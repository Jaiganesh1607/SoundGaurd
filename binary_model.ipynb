{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933414fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 STAGE 1: BINARY THREAT DETECTION TRAINING (FIXED VERSION)\n",
      "======================================================================\n",
      "📁 DATASET STRUCTURE CHECK:\n",
      "\n",
      "TRAIN Directory: C:\\Users\\Jaiganesh\\SoundGaurd\\data\\processed_data\\train\n",
      "  ✅ non_threat: 2100 files\n",
      "  ✅ threat/glass_break: 700 files\n",
      "  ✅ threat/scream: 700 files\n",
      "  ✅ threat/gunshot: 700 files\n",
      "  📊 TRAIN Total: 4200 files\n",
      "\n",
      "TEST Directory: C:\\Users\\Jaiganesh\\SoundGaurd\\data\\processed_data\\test\n",
      "  ✅ non_threat: 900 files\n",
      "  ✅ threat/glass_break: 300 files\n",
      "  ✅ threat/scream: 300 files\n",
      "  ✅ threat/gunshot: 300 files\n",
      "  📊 TEST Total: 1800 files\n",
      "\n",
      "📊 DATASET SUMMARY:\n",
      "   Train: 4200 files\n",
      "   Test: 1800 files\n",
      "   Grand Total: 6000 files\n",
      "\n",
      "======================================================================\n",
      "🔊 BUILDING TRAINING DATASET...\n",
      "[INFO] Found 2100 non-threat files\n",
      "[INFO] Found 700 glass_break files\n",
      "[INFO] Found 700 scream files\n",
      "[INFO] Found 700 gunshot files\n",
      "[INFO] Summary - Threats: 2100, Non-threats: 2100\n",
      "[INFO] Total files: 4200\n",
      "[INFO] Processing 4200 files...\n",
      "[INFO] Processed 500/4200 files...\n",
      "[INFO] Processed 1000/4200 files...\n",
      "[INFO] Processed 1500/4200 files...\n",
      "[INFO] Processed 2000/4200 files...\n",
      "[INFO] Processed 2500/4200 files...\n",
      "[INFO] Processed 3000/4200 files...\n",
      "[INFO] Processed 3500/4200 files...\n",
      "[INFO] Processed 4000/4200 files...\n",
      "[INFO] Successfully processed 4200 files\n",
      "[INFO] Feature shape: (4200, 240)\n",
      "Training Balance - Threats: 2100, Non-threats: 2100\n",
      "\n",
      "======================================================================\n",
      "🤖 TRAINING AI MODELS...\n",
      "\n",
      "🔄 Training LogisticRegression...\n",
      "\n",
      "=== Validation - LogisticRegression ===\n",
      "Accuracy: 0.9921\n",
      "Precision: 0.9844\n",
      "Recall: 1.0000\n",
      "F1-Score: 0.9921\n",
      "Macro F1: 0.9921\n",
      "\n",
      "Confusion Matrix:\n",
      "         Pred: Non-Threat  Threat\n",
      "Non-Threat:       310         5\n",
      "Threat:             0       315\n",
      "\n",
      "🔄 Training LinearSVM...\n",
      "\n",
      "=== Validation - LinearSVM ===\n",
      "Accuracy: 0.9921\n",
      "Precision: 0.9844\n",
      "Recall: 1.0000\n",
      "F1-Score: 0.9921\n",
      "Macro F1: 0.9921\n",
      "\n",
      "Confusion Matrix:\n",
      "         Pred: Non-Threat  Threat\n",
      "Non-Threat:       310         5\n",
      "Threat:             0       315\n",
      "\n",
      "🏆 BEST MODEL: LogisticRegression\n",
      "   Validation Macro F1: 0.9921\n",
      "\n",
      "🔄 Retraining LogisticRegression on full training set...\n",
      "\n",
      "======================================================================\n",
      "🧪 FINAL TEST EVALUATION...\n",
      "[INFO] Found 900 non-threat files\n",
      "[INFO] Found 300 glass_break files\n",
      "[INFO] Found 300 scream files\n",
      "[INFO] Found 300 gunshot files\n",
      "[INFO] Summary - Threats: 900, Non-threats: 900\n",
      "[INFO] Total files: 1800\n",
      "[INFO] Processing 1800 files...\n",
      "[INFO] Processed 500/1800 files...\n",
      "[INFO] Processed 1000/1800 files...\n",
      "[INFO] Processed 1500/1800 files...\n",
      "[INFO] Successfully processed 1800 files\n",
      "[INFO] Feature shape: (1800, 240)\n",
      "Test Balance - Threats: 900, Non-threats: 900\n",
      "\n",
      "=== 🎯 FINAL TEST RESULTS ===\n",
      "Accuracy: 0.9872\n",
      "Precision: 0.9944\n",
      "Recall: 0.9800\n",
      "F1-Score: 0.9871\n",
      "Macro F1: 0.9872\n",
      "\n",
      "Confusion Matrix:\n",
      "         Pred: Non-Threat  Threat\n",
      "Non-Threat:       895         5\n",
      "Threat:            18       882\n",
      "\n",
      "======================================================================\n",
      "✅ STAGE 1 COMPLETE!\n",
      "📁 Model saved: C:\\Users\\Jaiganesh\\SoundGaurd\\models_stage1\\stage1_binary_logisticregression.joblib\n",
      "📁 Config saved: C:\\Users\\Jaiganesh\\SoundGaurd\\models_stage1\\stage1_config.json\n",
      "🎯 Final Accuracy: 98.7%\n",
      "📊 Dataset Size: 4200 train + 1800 test = 6000 total files\n",
      "🎯 This model can detect threats with 98.7% accuracy!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from joblib import dump, load\n",
    "\n",
    "# =========================\n",
    "# CONFIG - CORRECTED PATHS\n",
    "# =========================\n",
    "DATASET_TRAIN_DIR = r\"C:\\Users\\Jaiganesh\\SoundGaurd\\data\\processed_data\\train\"\n",
    "DATASET_TEST_DIR  = r\"C:\\Users\\Jaiganesh\\SoundGaurd\\data\\processed_data\\test\"\n",
    "MODEL_DIR         = r\"C:\\Users\\Jaiganesh\\SoundGaurd\\models_stage1\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Feature extraction parameters\n",
    "SR = 22050\n",
    "N_MFCC = 40\n",
    "N_FFT = 1024\n",
    "WIN_LENGTH = 512\n",
    "HOP_LENGTH = 256\n",
    "USE_DELTAS = True\n",
    "POOL_STATS = [\"mean\", \"std\"]\n",
    "\n",
    "# Folder structure\n",
    "NON_THREAT_FOLDER = \"non_threat\"\n",
    "THREAT_PARENT_FOLDER = \"threat\"\n",
    "THREAT_SUBFOLDERS = [\"glass_break\", \"scream\", \"gunshot\"]\n",
    "\n",
    "# =========================\n",
    "# UTILITIES - FIXED VERSION\n",
    "# =========================\n",
    "def list_audio_files(root_dir):\n",
    "    \"\"\"List all audio files with binary threat/non-threat labels - FIXED VERSION\"\"\"\n",
    "    wavs = []\n",
    "    \n",
    "    if not os.path.isdir(root_dir):\n",
    "        print(f\"[ERROR] Root directory doesn't exist: {root_dir}\")\n",
    "        return wavs\n",
    "    \n",
    "    # Non-threat files (label = 0) - FIXED: avoid double counting\n",
    "    non_threat_path = os.path.join(root_dir, NON_THREAT_FOLDER)\n",
    "    if os.path.isdir(non_threat_path):\n",
    "        # Collect all unique WAV files (both .wav and .WAV)\n",
    "        all_non_threat_files = set()  # Use set to avoid duplicates\n",
    "        for ext in [\"*.wav\", \"*.WAV\"]:\n",
    "            pattern = os.path.join(non_threat_path, ext)\n",
    "            all_non_threat_files.update(glob.glob(pattern))\n",
    "        \n",
    "        all_non_threat_files = list(all_non_threat_files)  # Convert back to list\n",
    "        wavs.extend([(p, 0) for p in all_non_threat_files])\n",
    "        print(f\"[INFO] Found {len(all_non_threat_files)} non-threat files\")\n",
    "    else:\n",
    "        print(f\"[WARN] Non-threat directory missing: {non_threat_path}\")\n",
    "    \n",
    "    # Threat files (label = 1) - FIXED: avoid double counting\n",
    "    threat_parent_path = os.path.join(root_dir, THREAT_PARENT_FOLDER)\n",
    "    if os.path.isdir(threat_parent_path):\n",
    "        for subfolder in THREAT_SUBFOLDERS:\n",
    "            subfolder_path = os.path.join(threat_parent_path, subfolder)\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                # Collect all unique WAV files for this subfolder\n",
    "                all_threat_files = set()  # Use set to avoid duplicates\n",
    "                for ext in [\"*.wav\", \"*.WAV\"]:\n",
    "                    pattern = os.path.join(subfolder_path, ext)\n",
    "                    all_threat_files.update(glob.glob(pattern))\n",
    "                \n",
    "                all_threat_files = list(all_threat_files)  # Convert back to list\n",
    "                wavs.extend([(p, 1) for p in all_threat_files])\n",
    "                print(f\"[INFO] Found {len(all_threat_files)} {subfolder} files\")\n",
    "            else:\n",
    "                print(f\"[WARN] Threat subdirectory missing: {subfolder_path}\")\n",
    "    else:\n",
    "        print(f\"[WARN] Threat parent directory missing: {threat_parent_path}\")\n",
    "    \n",
    "    # Final summary\n",
    "    total_threats = len([w for w in wavs if w[1] == 1])\n",
    "    total_non_threats = len([w for w in wavs if w[1] == 0])\n",
    "    print(f\"[INFO] Summary - Threats: {total_threats}, Non-threats: {total_non_threats}\")\n",
    "    print(f\"[INFO] Total files: {len(wavs)}\")\n",
    "    \n",
    "    return wavs\n",
    "\n",
    "def extract_mfcc_features(wav_path):\n",
    "    \"\"\"Extract MFCC + deltas features\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        y, sr = sf.read(wav_path)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != SR:\n",
    "            y = librosa.resample(y, orig_sr=sr, target_sr=SR)\n",
    "        \n",
    "        # Extract MFCCs\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=y,\n",
    "            sr=SR,\n",
    "            n_mfcc=N_MFCC,\n",
    "            n_fft=N_FFT,\n",
    "            hop_length=HOP_LENGTH,\n",
    "            win_length=WIN_LENGTH\n",
    "        )\n",
    "        \n",
    "        feats = [mfcc]\n",
    "        \n",
    "        # Add deltas\n",
    "        if USE_DELTAS:\n",
    "            delta = librosa.feature.delta(mfcc, order=1)\n",
    "            delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "            feats.extend([delta, delta2])\n",
    "        \n",
    "        # Stack features (C x T)\n",
    "        F = np.vstack(feats)\n",
    "        \n",
    "        # Pool over time\n",
    "        pooled = []\n",
    "        for stat in POOL_STATS:\n",
    "            if stat == \"mean\":\n",
    "                pooled.append(np.mean(F, axis=1))\n",
    "            elif stat == \"std\":\n",
    "                pooled.append(np.std(F, axis=1))\n",
    "        \n",
    "        # Final feature vector (240 features: 120 coefficients × 2 stats)\n",
    "        pooled_vec = np.concatenate(pooled, axis=0)\n",
    "        return pooled_vec.astype(np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process {wav_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def build_dataset(root_dir):\n",
    "    \"\"\"Build feature matrix and labels\"\"\"\n",
    "    items = list_audio_files(root_dir)\n",
    "    \n",
    "    if len(items) == 0:\n",
    "        raise RuntimeError(f\"No audio files found in {root_dir}\")\n",
    "    \n",
    "    X, y, paths = [], [], []\n",
    "    failed_count = 0\n",
    "    \n",
    "    print(f\"[INFO] Processing {len(items)} files...\")\n",
    "    for i, (path, label) in enumerate(items):\n",
    "        if (i + 1) % 500 == 0:  # Progress indicator every 500 files\n",
    "            print(f\"[INFO] Processed {i + 1}/{len(items)} files...\")\n",
    "        \n",
    "        features = extract_mfcc_features(path)\n",
    "        if features is not None and np.all(np.isfinite(features)):\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "            paths.append(path)\n",
    "        else:\n",
    "            failed_count += 1\n",
    "    \n",
    "    if failed_count > 0:\n",
    "        print(f\"[WARN] Failed to process {failed_count} files\")\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        raise RuntimeError(f\"No valid features extracted from {len(items)} files\")\n",
    "    \n",
    "    X = np.vstack(X)\n",
    "    y = np.array(y, dtype=np.int64)\n",
    "    \n",
    "    print(f\"[INFO] Successfully processed {len(X)} files\")\n",
    "    print(f\"[INFO] Feature shape: {X.shape}\")\n",
    "    \n",
    "    return X, y, paths\n",
    "\n",
    "def evaluate_model(y_true, y_pred, title=\"\"):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
    "    macro_f1 = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")[2]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\"         Pred: Non-Threat  Threat\")\n",
    "    print(f\"Non-Threat:      {cm[0][0]:4d}      {cm[0][1]:4d}\")\n",
    "    print(f\"Threat:          {cm[1][0]:4d}      {cm[1][1]:4d}\")\n",
    "    \n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"macro_f1\": macro_f1}\n",
    "\n",
    "# =========================\n",
    "# MAIN TRAINING\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎯 STAGE 1: BINARY THREAT DETECTION TRAINING (FIXED VERSION)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Check folder structure first\n",
    "    print(\"📁 DATASET STRUCTURE CHECK:\")\n",
    "    total_train_files = 0\n",
    "    total_test_files = 0\n",
    "    \n",
    "    for split, split_dir in [(\"TRAIN\", DATASET_TRAIN_DIR), (\"TEST\", DATASET_TEST_DIR)]:\n",
    "        print(f\"\\n{split} Directory: {split_dir}\")\n",
    "        split_total = 0\n",
    "        \n",
    "        if os.path.isdir(split_dir):\n",
    "            # Check non-threat\n",
    "            non_threat_path = os.path.join(split_dir, NON_THREAT_FOLDER)\n",
    "            if os.path.isdir(non_threat_path):\n",
    "                # Count unique files (both .wav and .WAV)\n",
    "                wav_files = set(glob.glob(os.path.join(non_threat_path, \"*.wav\")))\n",
    "                wav_files.update(glob.glob(os.path.join(non_threat_path, \"*.WAV\")))\n",
    "                count = len(wav_files)\n",
    "                print(f\"  ✅ {NON_THREAT_FOLDER}: {count} files\")\n",
    "                split_total += count\n",
    "            else:\n",
    "                print(f\"  ❌ {NON_THREAT_FOLDER}: MISSING\")\n",
    "            \n",
    "            # Check threat subfolders\n",
    "            threat_path = os.path.join(split_dir, THREAT_PARENT_FOLDER)\n",
    "            if os.path.isdir(threat_path):\n",
    "                for subfolder in THREAT_SUBFOLDERS:\n",
    "                    subfolder_path = os.path.join(threat_path, subfolder)\n",
    "                    if os.path.isdir(subfolder_path):\n",
    "                        # Count unique files (both .wav and .WAV)\n",
    "                        wav_files = set(glob.glob(os.path.join(subfolder_path, \"*.wav\")))\n",
    "                        wav_files.update(glob.glob(os.path.join(subfolder_path, \"*.WAV\")))\n",
    "                        count = len(wav_files)\n",
    "                        print(f\"  ✅ threat/{subfolder}: {count} files\")\n",
    "                        split_total += count\n",
    "                    else:\n",
    "                        print(f\"  ❌ threat/{subfolder}: MISSING\")\n",
    "            else:\n",
    "                print(f\"  ❌ {THREAT_PARENT_FOLDER}: MISSING\")\n",
    "        else:\n",
    "            print(f\"  ❌ Directory doesn't exist!\")\n",
    "        \n",
    "        print(f\"  📊 {split} Total: {split_total} files\")\n",
    "        if split == \"TRAIN\":\n",
    "            total_train_files = split_total\n",
    "        else:\n",
    "            total_test_files = split_total\n",
    "    \n",
    "    print(f\"\\n📊 DATASET SUMMARY:\")\n",
    "    print(f\"   Train: {total_train_files} files\")\n",
    "    print(f\"   Test: {total_test_files} files\")\n",
    "    print(f\"   Grand Total: {total_train_files + total_test_files} files\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"🔊 BUILDING TRAINING DATASET...\")\n",
    "    X_train_full, y_train_full, train_paths = build_dataset(DATASET_TRAIN_DIR)\n",
    "    \n",
    "    threat_count = np.sum(y_train_full == 1)\n",
    "    non_threat_count = np.sum(y_train_full == 0)\n",
    "    print(f\"Training Balance - Threats: {threat_count}, Non-threats: {non_threat_count}\")\n",
    "    \n",
    "    # Validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, \n",
    "        test_size=0.15, \n",
    "        random_state=42, \n",
    "        stratify=y_train_full\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"🤖 TRAINING AI MODELS...\")\n",
    "    \n",
    "    # Models\n",
    "    logreg = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, C=1.0, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    linsvm = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LinearSVC(C=1.0, random_state=42, max_iter=2000))\n",
    "    ])\n",
    "    \n",
    "    # Train and validate\n",
    "    models = {\"LogisticRegression\": logreg, \"LinearSVM\": linsvm}\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n🔄 Training {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        results[name] = evaluate_model(y_val, y_pred, f\"Validation - {name}\")\n",
    "    \n",
    "    # Select best model\n",
    "    best_name = max(results, key=lambda x: results[x][\"macro_f1\"])\n",
    "    best_model = models[best_name]\n",
    "    \n",
    "    print(f\"\\n🏆 BEST MODEL: {best_name}\")\n",
    "    print(f\"   Validation Macro F1: {results[best_name]['macro_f1']:.4f}\")\n",
    "    \n",
    "    # Retrain on full training set\n",
    "    print(f\"\\n🔄 Retraining {best_name} on full training set...\")\n",
    "    best_model.fit(X_train_full, y_train_full)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"🧪 FINAL TEST EVALUATION...\")\n",
    "    X_test, y_test, test_paths = build_dataset(DATASET_TEST_DIR)\n",
    "    \n",
    "    test_threat_count = np.sum(y_test == 1)\n",
    "    test_non_threat_count = np.sum(y_test == 0)\n",
    "    print(f\"Test Balance - Threats: {test_threat_count}, Non-threats: {test_non_threat_count}\")\n",
    "    \n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    final_results = evaluate_model(y_test, y_test_pred, \"🎯 FINAL TEST RESULTS\")\n",
    "    \n",
    "    # Save model and config\n",
    "    model_filename = f\"stage1_binary_{best_name.lower()}.joblib\"\n",
    "    model_path = os.path.join(MODEL_DIR, model_filename)\n",
    "    dump(best_model, model_path)\n",
    "    \n",
    "    config = {\n",
    "        \"model_type\": best_name,\n",
    "        \"model_path\": model_path,\n",
    "        \"feature_params\": {\n",
    "            \"sr\": SR,\n",
    "            \"n_mfcc\": N_MFCC,\n",
    "            \"n_fft\": N_FFT,\n",
    "            \"win_length\": WIN_LENGTH,\n",
    "            \"hop_length\": HOP_LENGTH,\n",
    "            \"use_deltas\": USE_DELTAS,\n",
    "            \"pool_stats\": POOL_STATS\n",
    "        },\n",
    "        \"results\": final_results,\n",
    "        \"label_mapping\": {\"non_threat\": 0, \"threat\": 1},\n",
    "        \"dataset_info\": {\n",
    "            \"train_files\": len(X_train_full),\n",
    "            \"test_files\": len(X_test),\n",
    "            \"total_files\": len(X_train_full) + len(X_test)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = os.path.join(MODEL_DIR, \"stage1_config.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"✅ STAGE 1 COMPLETE!\")\n",
    "    print(f\"📁 Model saved: {model_path}\")\n",
    "    print(f\"📁 Config saved: {config_path}\")\n",
    "    print(f\"🎯 Final Accuracy: {final_results['accuracy']:.1%}\")\n",
    "    print(f\"📊 Dataset Size: {len(X_train_full)} train + {len(X_test)} test = {len(X_train_full) + len(X_test)} total files\")\n",
    "    print(f\"🎯 This model can detect threats with {final_results['accuracy']:.1%} accuracy!\")\n",
    "\n",
    "# Inference function for new files\n",
    "def predict_single_file(wav_path, model_path, config_path):\n",
    "    \"\"\"Predict threat/non-threat for a single WAV file\"\"\"\n",
    "    model = load(model_path)\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    features = extract_mfcc_features(wav_path)\n",
    "    if features is None:\n",
    "        return None, None\n",
    "    \n",
    "    features = features.reshape(1, -1)\n",
    "    prediction = model.predict(features)[0]\n",
    "    \n",
    "    # Get probability if available\n",
    "    probability = None\n",
    "    try:\n",
    "        if hasattr(model.named_steps['clf'], 'predict_proba'):\n",
    "            probability = model.named_steps['clf'].predict_proba(\n",
    "                model.named_steps['scaler'].transform(features)\n",
    "            )[0][1]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    label = \"threat\" if prediction == 1 else \"non_threat\"\n",
    "    return label, probability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
